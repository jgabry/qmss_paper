
\chapter{Review of Statistical Methods}
\label{lit_review}

Part~\ref{lit_review} begins with a summary of the ideas presented in \citeA{wawro_designing_2014} and then draws on the relevant literature to review the statistical theory underlying the model developed for the empirical example in Part~\ref{cox_katz}. Section \ref{hierarchical} is a brief review of hierarchical models. Gaussian Markov random fields (GMRF) are defined and discussed in \ref{gmrf}. Finally, \ref{star} discusses Bayesian structured additive regression (STAR) models. 

\section{Designing historical social scientific inquiry}
\label{wawro}

The recommendations of \citeA{wawro_designing_2014} are many, but the central theme is that the epistemological concerns of historians are not antithetical to a quantitative treatment but rather offer an opportunity to reflect on the shortcomings of traditional quantitative approaches and embrace different techniques that ``bridge the methodological divide" (p. 526). Indeed, greater sensitivity to these issues motivates the use of more interesting quantitative methods and naturally leads to a fully Bayesian approach.   Bayesian hierarchical models have the flexibility to accommodate parameter variation, account for unobserved heterogeneity, and, through prior probability distributions, can be further tailored to incorporate existing knowledge from previous studies and historically motivated hypotheses  \cite{wawro_designing_2014, goodrich_designing_2012}. Katznelson and Wawro suggest  semi-parametric mixed models -- in particular,  Bayesian structured additive regression (STAR) models -- and the flexible family of Gaussian Markov random fields priors for modeling interesting temporal and spatial relationships.

Accounting for variation over time (and potentially space) commonly involves estimating a large number of parameters relevant to the size of the available data, which can present a number of statistical challenges (e.g. under-identification). While Bayesian methods present their own unique set of burdens, many of which are highlighted throughout this thesis, they provide a more promising way forward than classical approaches estimated by maximum likelihood methods. 


\section{Hierarchical Bayesian models}
\label{hierarchical}

One of the many challenges of fitting models to data comprising multiple groupings is confronting the tradeoff between bias and variance. An analysis that disregards between-group heterogeneity can yield parameter estimates with low variance but high bias. Group-by-group analyses, on the other hand, can reduce bias at the expense of high-variance estimates \shortcite{jackman_bayesian_2009}. While complete pooling or no pooling of data across groups is sometimes called for, nonhierarchical models for hierarchical data tend to underfit or overfit the data \shortcite{gelman_bayesian_2013}. Hierarchical modeling provides a compromise by allowing parameters to vary by group at lower levels of the hierarchical while estimating population-level parameters at higher levels. 

For example, consider a simple binomial model for the number of survey respondents $y$ in favor of a particular policy. It might be reasonable to estimate separate underlying levels of support $\theta$ for the policy for each of $R$ different geographical regions. However, it is also likely that there is some dependence between the $\theta$s that should be incorporated into the joint probability model. This can be expressed with a prior distribution for the $\theta$s conditional on shared hyperparameters $\phi$, potentially also estimating $\phi$ from the data 

\begin{align*}
y_r | \theta_r &\sim {\rm Bin}(N_r, \theta_r), \quad r = 1, \dots, R \\
\theta_r  | \phi &\sim p(\theta_r | \phi) \\
\phi | \xi & \sim p(\phi | \xi)  \\
\vdots
\end{align*}

In theory there is no limit to the number of levels in the hierarchy -- if the value of $\xi$ is not fixed in the example above then it too must be modeled -- however, in practice the data are informative only up to a point (and there are computational challenges to be considered as well). 

Perhaps the most important feature of hierarchical models is that inference for each group-level parameter is informed not only by the group-specific information contained in the data but also by the other groups as well \shortcite{jackman_bayesian_2009}.\footnote{The assumption of exchangeability is important here, but a sufficient discussion of exchangeability is beyond the scope of this thesis. See \citeA{gelman_bayesian_2013}.} This is commonly referred to as borrowing strength. In the example above, for instance, inferences about each $\theta_r$ are informed by $y_r$ but also by $y_{-r}$ through $\phi$.\footnote{The notation $y_{-r}$ is used here to refer to all $y$'s other than $y_r$ (e.g. $y_{-1} =  \{y_2, y_3, \dots, y_R\}$ ).} 

The term {\it hierarchical} is a suitable designation for a wide (theoretically unlimited) family of models, from the simple binomial example above to models with regressions at multiple levels to the semi-parameteric STAR models used by Wawro and Katznelson and in this thesis. See \citeA{gelman_bayesian_2013} and \citeA{jackman_bayesian_2009} for more thorough and formal introductions to the topic of hierarchical modeling.  




\section{Gaussian Markov random fields}
\label{gmrf}

The prior distributions of particular interest in this thesis are known as Gaussian Markov random field (GMRF) priors. The literature on GMRFs is vast, as they are frequently used in image processing and spatial statistics, however GMRFs appear only rarely in quantitative social science, and \citeA{wawro_designing_2014} is the first example extending the applications of GMRFs to historical social scientific inquiry.\footnote{This is actually only somewhat true. Many special cases of GMRFs  (e.g. autoregressive processes) are commonly used by researchers applying statistics in many different fields including all of the social sciences. This is because GMRFs are  multivariate normal distributions. It is the particular use advocated by Wawro and Katznelson that is unfamiliar in the social sciences.} Before defining GMRFs it is first necessary to introduce some basic concepts from graph theory, particular the idea of an undirected graph. 

\subsection{Undirected graphs}
\label{undirected_graphs}

An undirected graph $\mathbf{G} = (V,E)$ is simply an ordered pair of sets containing the vertices (or nodes) and edges of the graph, respectively. In what follows assume that $V$ and $E$ are finite and let $e_{ij} \in E$ denote the element in $E$ corresponding to the edge connecting vertices $v_i$ and $v_j$ in $V$. The term {\it undirected} refers to the fact that the edges lack orientation.\footnote{Intuitively, it is helpful to think of the edges of an undirected graph as line segments with no implied direction, whereas the edges in a directed graph can be thought of as arrows.} The {\it neighbors} of node $v_j$ are all of the nodes that share a common edge with $v_j$. The notation $\partial^{\mathbf{G}}_j$ will be used for the {\it neighborhood} of node $v_j$ in graph  $\mathbf{G}$ and $\partial^\mathbf{G}$ for the set containing the neighborhoods for all nodes in $\mathbf{G}$.\footnote{$ \partial^\mathbf{G}_j = \{v_{i \neq j} \in V \,\vert\, e_{ij} \in E\}$ and $\partial^\mathbf{G} = \{\partial^\mathbf{G}_j \,\vert\, v_j \in V\}$.} For example, in Figure~\ref{fig:undirected_graphs}, $\partial^{\mtrx{A}}_1 = \{v_2, v_6\}$ and $\partial^{\mathbf{B}}_1 = \{v_2, v_3\}$.  For a more comprehensive discussion of undirected graphs in the context of GMRFs see \citeA{rue_gaussian_2005}. 

 \begin{figure}[htb]
\centering

\textsc{graph $\mtrx{A}$} \hspace{6cm} \textsc{graph $\mathbf{B}$} 

\vspace{.5cm}

\begin{tikzpicture}
\node[obs, fill=DarkSalmon] (r1) {$v_1$};
\node[obs, below left=of r1, fill=DarkSalmon] (r2) {$v_2$};
\node[obs, below=of r2, fill=DarkSalmon] (r3) {$v_3$};
\node[obs, below right=of r3, fill=DarkSalmon] (r4) {$v_4$};
\node[obs, above right=of r4, fill=DarkSalmon] (r5) {$v_5$};
\node[obs, above=of r5, fill=DarkSalmon] (r6) {$v_6$};
\edge [-, color=MidnightBlue, bend right=30] {r1} {r2} ;
\edge [-, color=MidnightBlue, bend left=30] {r1} {r6} ;
\edge [-, color=MidnightBlue] {r2} {r3} ;
\edge [-, color=MidnightBlue, bend right=30] {r3} {r4} ;
\edge [-, color=MidnightBlue, bend right=30] {r4} {r5} ;
\edge [-, color=MidnightBlue] {r5} {r6} ;
\end{tikzpicture}
%
 \hspace{4cm} 
 %
\begin{tikzpicture}
\node[obs, fill=SkyBlue] (r1) {$v_1$};
\node[obs, below right=of r1, fill=SkyBlue] (r2) {$v_2$};
\node[obs, below right=of r2,  fill=SkyBlue] (r3) {$v_3$};
\node[obs, below right=of r3,  fill=SkyBlue] (r4) {$v_4$};
\edge [-, color=DarkRed] {r1} {r2} ;
\edge [-, color=DarkRed, bend right=30] {r1} {r3} ;
\edge [-, color=DarkRed] {r2} {r3} ;
%\edge [-, color=DarkRed, bend left=30] {r2} {r4} ;
\edge [-, color=DarkRed] {r3} {r4} ;
\end{tikzpicture}
\vspace{.5cm}
\caption{Examples of simple undirected graphs. In \textsc{graph A} each node has two neighbors. In \textsc{graph B} node $v_4$ has only a single neighbor while the others each have two neighbors.}
\label{fig:undirected_graphs}
\end{figure}

 



\subsection{GMRFs}
Let $\theta \in \mathbb{R}^D$ have a multivariate normal distribution with probability density $p(\theta | \mu, \boldsymbol{\Sigma}) = \mathcal{N}_D (\theta | \mu, \boldsymbol{\Sigma})$. The random vector $\theta$ is said to form a GMRF with respect to the graph $\mathbf{G} = (V,E)$ if for each element in $\theta$ there is a corresponding node in $V$, and no edge $e_{ij}$ between nodes $v_i$ and $v_j$ \emph{if and only if} $\theta_i$ and $\theta_j$ are conditionally independent given all other elements of $\theta$ \shortcite{rue_gaussian_2005}.\footnote{$v_i \notin \partial^\mathbf{G}_{j} \iff \theta_i \bot \theta_j \mid \theta_{-ij}.$}\footnote{This definition is consistent with the definition of Markov random fields in general. The random variables $\theta_1, \dots, \theta_D$ obey the requisite Markov properties describing pairwise, local, and global conditional independencies \shortcite{rue_gaussian_2005}.} 

The relationship between $G$ and $\theta$ -- the information they provide about each other -- is fully contained in the covariance matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{D\times D}$, but it is not obvious from the individual elements of $\boldsymbol{\Sigma}$.\footnote{Nothing about the conditional independencies among the elements of $\theta$ can be inferred from the mean vector $\mu$.} It is more useful to instead consider the precision (inverse covariance) matrix $\mtrx{Q} =\boldsymbol{\Sigma}^{-1}$, for which it can be shown that conditional independence between $\theta_i$ and $\theta_j$ (no edge between nodes $v_i$ and $v_j$) always corresponds to a zero in cell $ij$ of the precision matrix, and vice-versa $(\forall i \neq j, \:\: v_i \in \partial_j \iff q_{ij} = 0)$.  For a simple proof see \citeA{rue_gaussian_2005}.\footnote{For intuition, the following interpretations of the elements of a precision matrix may also be helpful to keep in mind. For $i = j$ (the diagonal),  $q_{ij} = Var(\theta_i | \theta_{-i})$, the variance of $\theta_i$ conditional all the variables except $\theta_i$. For $i \neq j$ (the off-diagonal), $q_{ij}  = -Cov(\theta_i, \theta_j | \theta_{-ij}) $, the negative (flipped sign) covariance between $\theta_i$ and $\theta_j$ conditional on all $\theta$s except $i$ and $j$.  }






\section{Bayesian STAR models}
\label{star}

The structured additive regression (STAR) model replaces the linear predictor of the generalized linear model (GLM) with a semiparametric structured additive predictor

\begin{equation*}
\eta =  u^\intercal\gamma + f_1(x_1) + \ldots + f_j(x_j) + \ldots + f_J(x_J) =  u^\intercal\gamma + \sum_{j} f_j (x_j) ,
\end{equation*}

\noindent which can include nonlinear unknown functions of covariates as well as linear components and is flexible enough to include the assorted mixed effects models familiar to social scientists as special cases \shortcite{fahrmeir_bayesian_2001}. 

From the Bayesian perspective, the non-varying parameters $\gamma$ as well as the unknown functions $f_1, \dots, f_J$ are treated as random variables distributed according to priors that must be specified. For an unknown function $f_j(x_j)$ let $\mtrx{f}_j^{eval}$ denote the vector of function evaluations of $f_j$ at each of the $N$ observed values of $x_j$ 

\begin{equation*}
\mtrx{f}_j^{eval} = \left(f_j(x_{1j}), \dots f_j(x_{Nj})\right)^\intercal.  
 \end{equation*}
 
\noindent Then $\mathbf{f}_j^{eval}$ is treated as a random vector, which, following \citeA{brezger_generalized_2006}, can conveniently be expressed as the matrix product $\mtrx{M}_j \theta_j$ of a design matrix $\mtrx{M}_j \in \mathbb{R}^{N \times D}$ and a parameter vector $\theta_j$. The GMRF prior for each $\theta_j$ takes the general form

\begin{equation*}
p(\theta_j | \tau^2_j) 
\propto 
\frac{1}{\left(\tau^2_j\right)^{{\rm rank}(\mtrx{P})/2} }
\exp{\left\{-\frac{1}{2\tau^2_j} \theta_j^\intercal \mtrx{P} \theta_j\right\}}
\end{equation*}

\noindent where $\mtrx{P}$ is a penalty matrix whereby we operationalize our prior assumptions about the smoothness of the unknown function $f_j$.\footnote{See Appendix~\ref{AppendixA} for a discussion of ways to deal with the impropriety of this prior under certain conditions.}

To provide some intuition, suppose we are interested in learning about an effect with assumed variation over geographic regions.  A simple map with $R$ distinct regions $r_1, r_2, \dots r_R$ can be represented as an undirected graph $\mtrx{G}$ with vertices $V = \{v_1, v_2, \dots, v_R\}$ and edges $E$ connecting vertices corresponding to neighboring (or proximate) regions. A prior on a parameter vector $\theta = (\theta_1, \dots, \theta_r)$ can be constructed such that the matrix $\mtrx{P}$ has a zero in the $ij$th cell if and only if $\theta_i$ and $\theta_j$ (the parameters corresponding to regions $r_i$ and $r_j$) are assumed to be independent conditional on $\theta_{-ij}$. Then $\theta$ is a GMRF with respect to $G$ with $\boldsymbol{\Sigma}^{-1} = \mtrx{Q} = \mtrx{P}/\tau^2$. 

\subsection{The penalty matrix} 
\label{penalty_matrix}

More specifically, the penalty matrix $\mtrx{P}$ can be constructed as  $\mtrx{P} = \mtrx{D} - \mtrx{A}$, where $\mtrx{A}$ is a symmetric matrix with $a_{ij} = 1$ if temporal or spatial units $i$ and $j$ are considered neighbors (the associated graph $\mtrx{G}$ has an edge between nodes $i$ and $j$) and 0 otherwise, and $\mtrx{D}$ is a diagonal matrix such that $\forall i = j, \: d_{ij} = \sum_j a_{ij}$. The matrices $\mtrx{A}$ and $\mtrx{D}$ are commonly referred to as the adjacency and degree matrices because encoded in $\mtrx{A}$ are all (temporal and/or spatial) neighbor relationships -- the information in $\partial^\mtrx{G}$ in matrix form -- and the (diagonal) elements of $\mtrx{D}$ are equal to the number of neighbors (the degree) of each vertex in the graph $\mtrx{G}$. 

To illustrate why this form of $\mtrx{P}$ captures these particular assumptions, consider $N$ measurements of a variable $z$, with each measurement made at one of $T$ evenly spaced points in time. For simplicity, but without loss of generality,  assume that $z$ is a unit of time and there is exactly one measurement per time period.  The sequence $(z^{[t]})_{t=1}^T$ then corresponds to a grid of points on a line.  \citeA{fahrmeir_bayesian_2001} suggest several possible choices for a prior on a smooth function $f(z)$, the simplest of which is a first order random walk ($RW_1$) prior.  Under the $RW_1$ prior, the first differences $\Delta_t = f(z^{[t]}) - f(z^{[t-1]})$ are treated as independent and identically distributed standard normal random variables. 

While this formulation of the $RW_1$ prior is {\it directed}, conditioning also on $f(z^{[t+1]})$ -- one step into the future -- forms an undirected $RW_1$, where the neighbors of time $t$ are both $t-1$ and $t+1$.\footnote{Unless otherwise noted, the notation $RW$ will refer to an {\it undirected} random walk throughout the remainder of this thesis.} The associated graph $\mtrx{G}$ therefore has vertices $V=\{v_t : t=1,\dots,T\}$, each of which has two neighbors, with the exception of $v_1$ and $v_T$, which have one neighbor.\footnote{$\partial_t = \{v_{s} : t - 1 \leq s \leq t + 1\}.$}  The penalty matrix $\mtrx{P}$ corresponding to the $RW_1$ prior with equally spaced observations is the tridiagonal matrix

\begin{equation*}
\mtrx{P} = 
\begin{bmatrix}
1  	& -1 	& 		& 	& \\
-1  	& 2 	& -1 		& 	& \\
  	& -1 	& \ddots 	& \ddots	& \\
  	&  	& \ddots 	& 2 	& -1\\
  	&  	& 		& -1 	& 1\\
\end{bmatrix}
\end{equation*}

\noindent which can be derived by computing the difference of the appropriate degree and adjacency matrices \shortcite{brezger_generalized_2006}.\footnote{The construction of $\mtrx{P}$ as the matrix difference $\mtrx{D} - \mtrx{A}$ also allows for the estimation of a parameter $\omega \in [0,1]$, which, as a coefficient on $\mtrx{A}$, can be interpreted as representing the strength of spatial or temporal dependence \shortcite{rue_gaussian_2005}. The resulting precision matrix $\mtrx{Q}= (\mtrx{D} - \omega \mtrx{A})/\tau^2$ is the defining feature of the conditional autoregressive (CAR) model.  When $\omega = 1$, and thus $\mtrx{P}= \mtrx{D} - \mtrx{A}$, the model is sometimes referred to as an intrinsic CAR model. In general, CAR and (G)MRF refer to models with the same structure but different parameterizations. In the literature, the term CAR tends to be reserved for the model when expressed in terms of conditional distributions (for each random variable given its neighborhood) rather than the joint distribution \shortcite{banerjee_hierarchical_2004}. See also Appendix~\ref{AppendixA} for more details. \label{footnote_car}} The $RW_1$ can be extended to an $RW_2$ prior by also considering the measurements $z^{[t-2]}$  and $z^{[t+2]}$ to be neighbors of $z^{[t]}$. 


\subsection{Hyperpriors}
\label{hyperpriors}

To complete the hierarchical model requires also specifying hyperpriors $p(\tau_j^2)$.  Assigning a prior distribution to the variance hyperparameter $\tau_j^2$ allows for the simultaneous estimation of a smoothing function $f_j$ and the amount of smoothness. \citeA{fahrmeir_bayesian_2001} and \citeA{brezger_bayesx:_2005} recommend  inverse-gamma priors for the $\tau_j^2$. However, in light of concerns about this type of inverse-gamma prior raised by \citeA{gelman_prior_2006}, in this thesis several priors for $\tau^2_j$ are used and compared to check the degree to which the results are sensitive to the choice of prior.

