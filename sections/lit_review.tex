
\chapter{Review of Statistical Methods}
\label{lit_review}

%\section{Designing historical social scientific inquiry}
%\label{wawro}

%This orientation to how particular surrounding circum- stances affect the play of causal relations implies close at- tention to the particularity of locations and moments, for interaction of factors is shaped by noteworthy elements at distinct times and places.

Katznelson and Wawro view the epistemological concerns of historians not as 
antithetical to a quantitative treatment, but rather as motivation for reflecting on the 
shortcomings of traditional quantitative approaches and prioritizing the development
of novel techniques for traversing the methodological and philosophical divide 
\shortcite{wawro_designing_2014}. 
Indeed, greater sensitivity to periodicity, temporality, context and specificity requires  
more versatile quantitative methods and naturally leads to a fully Bayesian approach.%
\footnote{ 
{\it Periodicity} is not used here in the mathematical sense, but rather to refer to how relationships 
between variables are influenced differently by break points in history at particular locations and times. 
{\it Temporality} refers to how the sequential nature of events in time affects structure and agency, 
{\it specificity} refers to the uniqueness of conditions, and
{\it context} is the broader environment -- the surrounding circumstances -- within which events occur
\shortcite{wawro_designing_2014}.
} %

Accounting for variation over time (and possibly other dimensions of interest) often entails 
estimating a large number of parameters relative to the size of the available data, which can lead 
to a number of statistical challenges (e.g. under-identification). In this regard, although Bayesian 
methods often require confronting a unique set of computational burdens -- many of which are 
highlighted throughout this thesis -- they provide a more promising way forward than 
classical approaches estimated by maximum likelihood methods. 

Bayesian hierarchical models have the flexibility to accommodate parameter 
variation over multiple dimensions, account for unobserved heterogeneity, and, 
through prior probability distributions, can be further tailored to incorporate existing 
knowledge from previous studies and historically motivated hypotheses 
\shortcite{wawro_designing_2014, goodrich_designing_2012}. 
Katznelson and Wawro advocate structured additive regression (STAR) 
and the flexible family of Gaussian Markov random field priors for modeling interesting 
temporal and spatial relationships. 

The following sections draw on pertinent literature to review the statistical theory underlying 
the model developed for the empirical example in Part~\ref{cox_katz}, which is a special case 
of the STAR family of models endorsed by Wawro and Katznelson. Section \ref{hierarchical} 
is a brief review of hierarchical models. Gaussian Markov random fields (GMRF) are defined 
and discussed in \ref{gmrf}. Finally, \ref{star} introduces the STAR model from a Bayesian perspective. 



\section{Hierarchical Bayesian models}
\label{hierarchical}

One of the many challenges of fitting models to data comprising multiple groupings is 
confronting the tradeoff between bias and variance. An analysis that disregards between-group 
heterogeneity can yield parameter estimates with low variance but high bias. Group-by-group 
analyses, on the other hand, can reduce bias at the expense of high-variance estimates \shortcite{jackman_bayesian_2009}. While complete pooling or no pooling of data across 
groups is sometimes called for, nonhierarchical models for hierarchical data tend to underfit 
or overfit the data \shortcite{gelman_bayesian_2013}. Hierarchical modeling provides a 
compromise by allowing parameters to vary by group at lower levels of the hierarchy while 
estimating population-level parameters at higher levels. 

A simple hierarchical model for data $y = (y_1, \dots, y_N)$ organizes observations into $J \leq N$ 
groups, with group-level parameters $\theta = (\theta_1, \dots, \theta_J)$ and global parameter(s) $\xi$ 
(see Figure~\ref{fig:hierarchical_model}) and can be written

\begin{align*}
y_n &\sim p(y_n | \theta_{j[n]}) \\
\theta_j &\sim p(\theta_j | \xi) \\
\xi &\sim p(\xi),
\end{align*}

\noindent where $j[n]$ is the group pertaining to the $n$th data point. Observations are assumed 
to be conditionally independent given the group-level parameters, and the group-level parameters 
conditionally independent given the global parameters. The joint posterior distribution for the 
parameters and hyperparameters then factors nicely as

\begin{equation*}
 p(\theta, \xi | y) \propto p(\xi)p(\theta | \xi)p(y|\theta) = p(\xi) \prod_{j=1}^J p(\theta_j | \xi) \prod_{n=1}^N p(y_n | \theta_{j[n]}). 
\end{equation*} 


\begin{figure}
\centering
\begin{forest}
for tree={
  edge={->,>=latex},
  parent anchor=south,
  child anchor=north,
  content format={\ensuremath{\forestoption{content}}},
  }
[{\xi},name=level0
  [\theta_{1},name=level1
    [\{y_{n : j[n] = 1}\},name=level2]
  ]
  [\theta_{2}
    [\{y_{n : j[n] = 2}\}]
  ]
  [\cdots
    [\cdots]
  ]
  [\theta_{J-1}
    [\{y_{n : j[n] = J-1}\}]
  ]  
  [\theta_{J}
    [\{y_{n : j[n] = J}\}]
  ]
]
%\foreach \Name/\Label in {level2/$p(y_{n}|\theta_{j[n]})$,level1/$p(\theta_j|\phi)$,level0/$p(\phi)$}
%  \node[anchor=west] at ([xshift=60pt]\Name) {\Label};   
\end{forest}
\caption{A simple hierarchical structure with $N$ observations $y_1, \dots, y_N$ organized in 
to $J$ groups, with group-level parameters $\theta_1, \dots, \theta_J$, and global hyperparameter(s) 
$\xi$. The notation $\{y_{n : j[n] = j}\}$ is used for the set of observations belonging to the $j$th group.}
\label{fig:hierarchical_model} 
\end{figure}



%parameters $\theta_1
%Hierarchical models [1] are defined by the organization of a model?s parameters into exchangeable groups, and the resulting conditional independencies between those groups. simple hierarchical model 

The term {\it hierarchical} is a suitable designation for a diverse family of 
models. When parameters are grouped into multiple different hierarchical structures 
it is common to refer to the resulting models as {\it multilevel models} \shortcite{gelman_arm_2007}.  
For example, consider a scenario in which survey respondents from many different
geographical regions are asked if they favor a particular policy $P$. One 
approach for estimating the level of underlying support for the policy in each region 
$r = 1, 2, \dots, R$ would be to estimate separate models for the $R$ regions. However, 
such an approach would ignore the relationship between support for $P$ across regions, 
which should be incorporated into the joint probability model. In the United States, for example, 
estimating separate models would impose the assumption that support for $P$ in, say, New 
England is entirely uninformative about support for $P$ in all other regions of the country. A 
hierarchical framework allows for a more realistic model in which support for $P$ can vary 
across regions while accounting for interregional dependence.      

For the $n$th observation, let $r[n]$ denote the respondent's region and $p_n = {\rm Pr}(y_n = 1)$ 
be the probability the respondent expresses support for policy $P$. For simplicity, assume that there 
is a single individual-level predictor $x$ (e.g. income) and a single region-level predictor $w$ 
(e.g. average income). The hierarchical (multilevel) model

%
\begin{align*}
p_n &= {\rm logit}^{-1}\left(\alpha_{r[n]} + \beta_{r[n]} x_n\right), \quad \text{for } n = 1, \dots, N \\[5pt]
 \alpha_r &= \gamma^\alpha_0 + \gamma^\alpha_1 w_r + \epsilon^\alpha_r, \quad \text{ for } r = 1, \dots, R \\[5pt]
  \beta_r &= \gamma^\beta_0 + \gamma^\beta_1 w_r + \epsilon^\beta_r, \qquad\qquad \vdots  \\[5pt]
   \epsilon^\alpha_r &\sim \mathcal{N}(0, \tau^2_\alpha), \\[5pt]
   \epsilon^\beta_r &\sim \mathcal{N}(0, \tau^2_\beta).
\end{align*}

\noindent is what \citeA{gelman_arm_2007} refer to as a {\it varying-intercept}, 
{\it varying-slope} model. That is, both the intercepts and the slopes -- the effects 
associated with the variable $x$-- vary by group (region, in this example) and 
are given probability models conditional on a further level of parameters.\footnote{
Typically a parameter representing the correlation between the error terms 
$\epsilon_r^\alpha$ and $\epsilon_r^\beta$ is also estimated.} 
These so-called hyperparameters 
%The model above estimates an $R$-vector of intercepts $\alpha = (\alpha_1, \dots, \alpha_R)$ 
%as well as an $R$-vector of slopes $\beta = (\beta_1, \dots, \beta_R)$. The hyperparameters
%$(\gamma^\alpha_0, \gamma^\alpha_1 \gamma^\beta_0, \gamma^\beta_1, \tau^2_\alpha, \tau^2_\beta)$ 
are also estimated from the data and thus require prior distributions (hyperpriors) to be specified. 
In theory there is no limit to the number 
of levels in the hierarchy -- if the parameters of the hyperpriors are not fixed then they too are modeled. 
In practice, the appropriate depth of the hierarchy depends on both the hierarchical 
structures present in the data and computational considerations \shortcite{gelman_bayesian_2013}. 

Perhaps the most important feature of hierarchical models is that inference for each group-level 
parameter is informed not only by the group-specific information contained in the data but also 
by the data for other groups as well. This is commonly referred to as {\it borrowing strength} 
or {\it shrinkage}. 

In the example in Figure~\ref{fig:hierarchical_model}, 
inference for $\theta_j$ is directly informed by the observations 
 $\{y_{n : j[n] = j}\}$ in group $j$ but also indirectly by the observations  
 $\{y_{n : j[n] \neq j}\}$ from all other groups due to the common 
dependence on the global parameter $\xi$. In the survey example, the estimates for each 
$\alpha_r$, for instance, are informed not only by the data from region $r$ but also by the 
observations from all other regions through the shared hyperparameters. This describes
the general way in which the levels in a hierarchical model are connected in the data. 
The unit (individual) level observations are the points from which the higher-level group 
distributions are estimated. That is, {\it strength} is {\it borrowed} from all groups when estimating 
the parameter(s) corresponding to each individual group. As a consequence the estimates for the 
group-level parameters {\it shrink} toward their common mean, with the amount of shrinkage a 
function of the distance from that mean 
\shortcite{jackman_bayesian_2009, lipsky_shrinkage_2010}.\footnote{See \citeA{gelman_arm_2007} 
and \citeA{jackman_bayesian_2009} for more thorough and formal treatments of hierarchical modeling. 
Also see \citeA{lipsky_shrinkage_2010} for an intuitive explanation of shrinkage in multilevel models.}



\section{Gaussian Markov random fields}
\label{gmrf}

The prior distributions of particular interest in this thesis are known as Gaussian Markov random 
field (GMRF) priors. The literature on GMRFs is vast, as they are frequently used in image processing 
and spatial statistics, however GMRFs appear only rarely in quantitative social science, and 
\citeA{wawro_designing_2014} is the first example extending the applications of GMRFs to historical 
social scientific inquiry.\footnote{This is actually only somewhat true. Many special cases of GMRFs  
(e.g. autoregressive processes) are commonly used by researchers applying statistics in many different 
fields including all of the social sciences. This is because GMRFs are  multivariate normal distributions. 
It is the particular use advocated by Wawro and Katznelson that is unfamiliar in the social sciences.} 
Before defining GMRFs it is first necessary to introduce some concepts from graph theory, particularly 
undirected graphs. 

\subsection{Preliminaries: undirected graphs}
\label{undirected_graphs}

An undirected graph $\mathbf{G} = (V,E)$ is simply an ordered pair of sets containing the 
vertices (or nodes) and edges of the graph, respectively. In what follows assume that $V$ 
and $E$ are finite and let $\bar{e}_{ij} \in E$ denote the element in $E$ corresponding to the 
edge connecting vertices $\nu_i$ and $\nu_j$ in $V$. The term {\it undirected} refers to the fact 
that the edges lack orientation.\footnote{Intuitively, it is helpful to think of the edges of an 
undirected graph as line segments with no implied direction, whereas the edges in a directed 
graph can be thought of as arrows.} The {\it neighbors} of node $\nu_j$ are all nodes that 
share a common edge with $\nu_j$. The notation $\partial^{\mathbf{G}}_j$ will be used for the 
{\it neighborhood} of node $\nu_j$ in graph  $\mathbf{G}$ and $\partial^\mathbf{G}$ for the set 
containing the neighborhoods for all nodes in $\mathbf{G}$.\footnote{$ \partial^\mathbf{G}_j = 
\{\nu_{i \neq j} \in V : \bar{e}_{ij} \in E\}$ and 
$\partial^\mathbf{G} = \{\partial^\mathbf{G}_j : \nu_j \in V\}$.} 
For example, in Figure~\ref{fig:undirected_graphs}, 
$\partial^{\mtrx{A}}_1 = \{\nu_2, \nu_6\}$ and $\partial^{\mathbf{B}}_1 = \{\nu_2, \nu_3\}$. 
Also in Figure~\ref{fig:undirected_graphs}, the node $\nu_{11}$ and its neighborhood 
$\partial^\mtrx{G}_{11}$ are highlighted in \textsc{graph $\mtrx{C}$}. 
For a more comprehensive treatment of undirected graphs in the context of GMRFs see 
\citeA{rue_gaussian_2005}. 

\begin{figure}[htb]
\vspace{1cm}

\textsc{graph $\mtrx{A}$} \hspace{3cm} \textsc{graph $\mtrx{B}$} \hspace{4.5cm} \textsc{graph $\mtrx{C}$} \hfill 

\centering


\vspace{.5cm}

\begin{tikzpicture}
\node[obs, color=PineGreen, text=white] (r1) {$\nu_1$};
\node[obs, below left=of r1, color=PineGreen, text=white] (r2) {$\nu_2$};
\node[obs, below=of r2, color=PineGreen, text=white] (r3) {$\nu_3$};
\node[obs, below right=of r3, color=PineGreen, text=white] (r4) {$\nu_4$};
\node[obs, above right=of r4, color=PineGreen, text=white] (r5) {$\nu_5$};
\node[obs, above=of r5, color=PineGreen, text=white] (r6) {$\nu_6$};
\edge [-, color=Melon, bend right=30] {r1} {r2} ;
\edge [-, color=Melon, bend left=30] {r1} {r6} ;
\edge [-, color=Melon] {r2} {r3} ;
\edge [-, color=Melon, bend right=30] {r3} {r4} ;
\edge [-, color=Melon, bend right=30] {r4} {r5} ;
\edge [-, color=Melon] {r5} {r6} ;
\end{tikzpicture}
%
 \hspace{1cm} 
 %
\begin{tikzpicture}
\node[obs, color=PineGreen, text=white] (r1) {$\nu_1$};
\node[obs, below right=of r1, color=PineGreen, text=white] (r2) {$\nu_2$};
\node[obs, below right=of r2,  color=PineGreen, text=white] (r3) {$\nu_3$};
\node[obs, below right=of r3,  color=PineGreen, text=white] (r4) {$\nu_4$};
\edge [-, color=Melon] {r1} {r2} ;
\edge [-, color=Melon, bend right=30] {r1} {r3} ;
\edge [-, color=Melon] {r2} {r3} ;
%\edge [-, color=DarkRed, bend left=30] {r2} {r4} ;
\edge [-, color=Melon] {r3} {r4} ;
\end{tikzpicture}
%
 \hspace{1cm} 
 %
\begin{tikzpicture}
\node[obs, color=PineGreen, fill = white, text=PineGreen] (r1) {$\nu_1$};
\node[obs, right=of r1, color=gray, fill=white] (r2) {$\nu_2$};
\node[obs, right=of r2,  color=gray, fill=white] (r3) {$\nu_3$};
\node[obs, right=of r3,  color=gray, fill=white] (r4) {$\nu_4$};
\node[obs, below=of r1, color=gray, fill=white] (r5) {$\nu_5$};
\node[obs, below=of r2, color=PineGreen,  fill=white, text=PineGreen] (r6) {$\nu_6$};
\node[obs, below=of r3,  color=PineGreen, fill=white, text=PineGreen] (r7) {$\nu_7$};
\node[obs, below=of r4,  color=gray, fill=white] (r8) {$\nu_8$};
\node[obs, below=of r5, color=gray, fill=white] (r9) {$\nu_9$};
\node[obs, below=of r6,  color=PineGreen, fill=white, text=PineGreen] (r10) {$\nu_{10}$};
\node[obs, below=of r7,  color=PineGreen, text=white] (r11) {$\nu_{11}$};
\node[obs, below=of r8,  color=PineGreen, fill=white, text=PineGreen] (r12) {$\nu_{12}$};
\edge [-, color=gray] {r1} {r2} ; \edge [-, color=gray] {r1} {r5} ;
\edge [-, color=gray] {r2} {r3} ; \edge [-, color=gray] {r2} {r6} ;
\edge [-, color=gray] {r3} {r4} ; \edge [-, color=gray] {r3} {r7} ;
\edge [-, color=gray] {r4} {r8} ; 
\edge [-, color=gray] {r5} {r6} ; \edge [-, color=gray] {r5} {r9} ;
\edge [-, color=gray] {r6} {r7} ; \edge [-, color=gray] {r6} {r10} ;
\edge [-, color=gray] {r7} {r8} ; \edge [-, color=Melon] {r7} {r11} ;
\edge [-, color=gray] {r8} {r12} ; 
\edge [-, color=gray] {r9} {r10} ; \edge [-, color=Melon] {r10} {r11} ; \edge [-, color=Melon] {r11} {r12} ; 

\edge [-, color=gray] {r4} {r7} ; \edge [-, color=gray] {r2} {r7} ; 
\edge [-, color=gray] {r5} {r10} ; \edge [-, color=Melon] {r6} {r11} ; 
\edge [-, color=gray, bend right=30] {r4} {r12} ; \edge [-, color=Melon, bend left=30] {r1} {r11} ;

\end{tikzpicture}
\vspace{.5cm}
\caption{Examples of simple undirected graphs. In \textsc{graph A} each node has two neighbors. 
In \textsc{graph B} node $\nu_4$ has only a single neighbor while the others each have two neighbors. 
In \textsc{graph C} the node $\nu_{11}$ and its neighborhood $\partial^\mtrx{G}_{11}$ are highlighted.}
\label{fig:undirected_graphs}
\end{figure}

 



\subsection{Definition}

Let $\theta \in \mathbb{R}^D$ have a multivariate normal distribution with probability 
density $p(\theta | \mu, \boldsymbol{\Sigma}) = \mathcal{N}_D (\theta | \mu, \boldsymbol{\Sigma})$. 
The random vector $\theta$ is said to form a GMRF with respect to the graph $\mathbf{G} = (V,E)$ if 
for each element in $\theta$ there is a corresponding node in $V$, and no edge $\bar{e}_{ij}$ between 
nodes $\nu_i$ and $\nu_j$ \emph{if and only if} $\theta_i$ and $\theta_j$ are conditionally independent 
given all other elements of $\theta$ \shortcite{rue_gaussian_2005}.\footnote{$\nu_i \notin 
\partial^\mathbf{G}_{j} \iff \theta_i \bot \theta_j \mid \theta_{-ij}.$}\footnote{This definition is consistent 
with the definition of Markov random fields in general. The random variables $\theta_1, \dots, \theta_D$ 
obey the requisite Markov properties describing pairwise, local, and global conditional independencies \shortcite{rue_gaussian_2005}.} The conditional distribution $p(\theta_j | \theta_{-j})$, where $\theta_{-j}$
denotes all elements of $\theta$ except for $\theta_j$, then simplifies to  $p(\theta_j |  \theta_{i : \nu_i \in \partial^\mtrx{G}_j})$, which is the distribution of $\theta_j$ conditional only on its neighbors. 

The relationship between $G$ and $\theta$ -- the information they provide about each other -- is fully 
contained in the covariance matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{D\times D}$, but it is not 
obvious from the individual elements of $\boldsymbol{\Sigma}$.\footnote{Nothing about the conditional 
independencies among the elements of $\theta$ can be inferred from the mean vector $\mu$.} It is 
more useful to instead consider the precision (inverse covariance) matrix 
$\mtrx{Q} =\boldsymbol{\Sigma}^{-1}$, for which it can be shown that conditional independence between 
$\theta_i$ and $\theta_j$ (no edge between nodes $\nu_i$ and $\nu_j$) always corresponds to a zero in 
cell $ij$ of the precision matrix, and vice-versa.\footnote{
$\forall i \neq j, \:\: \nu_i \notin \partial_j \iff q_{ij} = 0$, or equivalently $\bar{e}_{ij} \in E \iff q_{ij} \neq 0$.}  
For a simple proof see \citeA{rue_gaussian_2005}.\footnote{For intuition, the following interpretations of 
the elements of a precision matrix may also be helpful to keep in mind. For $i = j$ (the diagonal),  
$q_{ij} = Var(\theta_i | \theta_{-i})$, the variance of $\theta_i$ conditional all the variables except 
$\theta_i$. For $i \neq j$ (the off-diagonal), $q_{ij}  = -Cov(\theta_i, \theta_j | \theta_{-ij}) $, the negative 
(flipped sign) covariance between $\theta_i$ and $\theta_j$ conditional on all $\theta$s except $i$ and $j$.}


\section{Hierarchical Bayesian STAR models}
\label{star}

\subsection{Definition}

\shortciteA{fahrmeir_regression_2013} describe structured additive regression (STAR) 
as a unified framework for formulating regression models when simple functional forms are 
insufficient for describing the relationship between response variables and predictors. 
The STAR model replaces the linear predictor of the 
generalized linear model with a structured additive predictor

\begin{equation*}
  \eta_n =  w_n^\intercal\gamma + f_1(x_{n1}) + \ldots + f_j(x_{nj}) + \ldots + f_J(x_{nJ}), \quad n = 1, \dots, N, 
\end{equation*}

\noindent which can incorporate linear components, nonlinear unknown functions of covariates, 
and is flexible enough to encompass a broad collection of models as special cases, among them the 
assorted mixed effects models familiar to social scientists 
\shortcite{fahrmeir_bayesian_2001, fahrmeir_regression_2013}.\footnote{\citeA{brezger_star_2005} 
provide a detailed list of the various possible roles for the functions $f_j$, which includes ``\dots usual 
nonlinear effects of continuous covariates, time trends and seasonal effects, two-dimensional surfaces, 
varying coefficient terms, i.i.d. random intercepts and slopes, spatially correlated effects, and geographically 
weighted regression" (p. 2). The varying-intercept, varying-slope models described in \ref{hierarchical} 
are examples of mixed effects models and are also special cases of the STAR model. Other familiar 
special cases include generalized additive mixed models.} 

From the Bayesian perspective, the non-varying parameters $\gamma$ as well as the 
functions $f_1, \dots, f_J$ are treated as random variables distributed according to priors that must 
be specified. For an unknown function $f_j$ let $\mtrx{f}_j^{eval}$ denote the vector of function 
evaluations of $f_j$ at each of the $N$ observed values of the variable $x_j$, 

\begin{equation*}
\mtrx{f}_j^{eval} = \left(f_j(x_{1j}), \dots, f_j(x_{nj}), \dots, f_j(x_{Nj})\right)^\intercal.  
 \end{equation*}
 
\noindent Then $\mathbf{f}_j^{eval}$ is a random vector, which, following 
\citeA{brezger_generalized_2006}, can conveniently be expressed as a linear model 
$\mathbf{f}_j^{eval} = \mtrx{M}_j \theta_j$, where $\mtrx{M}_j$ is a design matrix and 
$\theta_j$ a vector of coefficients \shortcite{fahrmeir_regression_2013}. 

The prior for each $\theta_j$  takes the general form

\begin{equation*}
p(\theta_j | \tau^2_j) 
\propto 
\left(\frac{1}{\tau_j^2} \right)^{{\rm rank}(\mtrx{P})/2}
\exp{\left\{-\frac{1}{2\tau^2_j} \theta_j^\intercal \mtrx{P}_j \theta_j\right\}},
\end{equation*}

\noindent where $\mtrx{P}$ is a penalty matrix which operationalizes prior assumptions 
about the smoothness of the function $f_j$, and the hyperparameter $\tau^2_j$
informs the amount of smoothing.\footnote{See Appendix A % Appendix~\ref{AppendixA} 
for a discussion of ways to deal with the impropriety of this prior under certain conditions.}

To provide some intuition, consider a model where the intercept varies by geographic region. 
A simple map with $R$ distinct regions $r_1, r_2, \dots r_R$ can be represented as an undirected graph 
$\mtrx{G}$ with vertices $V = \{\nu_1, \nu_2, \dots, \nu_R\}$ and edges $E$ connecting vertices corresponding 
to neighboring (or proximate) regions. Each region has a unique regression coefficient $f(r_k) = \alpha_k$,
$k = 1, \dots, R$, and the vector of function evaluations is $\mathbf{f}^{eval} = \mtrx{M} \alpha$,
where $m_{i,j} = 1$ if the $i$th observation is from the $j$th region and 0 otherwise. The associated 
coefficient vector is $\alpha = (\alpha_1, \dots, \alpha_R)$ and the matrix $\mtrx{P}$ has a zero in  
cell $p_{ij}$ if and only if $\alpha_i$ and $\alpha_j$ are assumed to be independent conditional on 
$\alpha_{-ij}$. 

\subsection{The penalty matrix} 
\label{penalty_matrix}

The form of the penalty matrix differentiates between many of the models in the STAR family. 
For the GMRF priors, the penalty matrix can be constructed as $\mtrx{P} = \mtrx{D} - \mtrx{A}$, 
where $\mtrx{A}$ is a symmetric matrix with $a_{ij} = 1$ if temporal or spatial units $i$ and $j$ 
are considered neighbors and 0 otherwise, and $\mtrx{D}$ is a diagonal matrix such that 
$\forall i = j, \: d_{ij} = \sum_j a_{ij}$. The matrices $\mtrx{A}$ and $\mtrx{D}$ are 
commonly referred to as the adjacency and degree matrices because encoded in 
$\mtrx{A}$ are all (temporal and/or spatial) neighbor relationships -- the information 
in $\partial^\mtrx{G}$ in matrix form -- and the (diagonal) elements of $\mtrx{D}$ 
are equal to the number of neighbors (the degree) of each vertex in the 
graph $\mtrx{G}$. 

To illustrate why this form of $\mtrx{P}$ captures these particular assumptions, 
consider $N$ measurements of a variable $z$, with each measurement made at 
one of $T$ evenly spaced points in time. For simplicity, but without loss of generality,  
assume there is exactly one measurement per time period (i.e., $N = T$).  
The sequence $(z^{[t]})_{t=1}^T$ then corresponds to a grid of points on a line. 
\citeA{fahrmeir_bayesian_2001} suggest several possible choices for a prior on a smooth
function $f(z)$, the simplest of which is a first order random walk ($RW_1$) prior.  Under 
the $RW_1$ prior, the first differences $\Delta_t = f(z^{[t]}) - f(z^{[t-1]})$ are treated as 
independent and identically distributed standard normal random variables. 

While this formulation of the $RW_1$ prior is {\it directed}, conditioning also on 
$f(z^{[t+1]})$ -- one step into the future -- forms an undirected $RW_1$, where 
the neighbors of time $t$ are both $t-1$ and $t+1$.\footnote{Unless otherwise 
noted, the notation $RW$ will refer to an {\it undirected} random walk throughout 
the remainder of this thesis.} The associated graph $\mtrx{G}$ therefore has vertices 
$V=\{\nu_t : t=1,\dots,T\}$, each of which has two neighbors, with the exception of 
$\nu_1$ and $\nu_T$, which have one neighbor.\footnote{$\partial_t = \{\nu_{s} : 
t - 1 \leq s \leq t + 1\}.$}  The penalty matrix $\mtrx{P}$ corresponding to the $RW_1$ 
prior with equally spaced observations is the tridiagonal matrix

\begin{equation*}
\mtrx{P} = 
\begin{bmatrix}
1  	& -1 	& 		& 	& \\
-1  	& 2 	& -1 		& 	& \\
  	& -1 & \ddots 	& \ddots	& \\
  	&  	& \ddots 	& 2 	& -1\\
  	&  	& 		& -1 	& 1\\
\end{bmatrix},
\end{equation*}

\noindent which can be derived by computing the difference of the appropriate degree 
and adjacency matrices \shortcite{brezger_generalized_2006}.\footnote{The construction 
of $\mtrx{P}$ as the matrix difference $\mtrx{D} - \mtrx{A}$ also allows for the estimation 
of a parameter $\omega \in [0,1]$, which, as a coefficient on $\mtrx{A}$, can be 
interpreted as representing the strength of spatial or temporal dependence 
\shortcite{rue_gaussian_2005}. The resulting precision matrix 
$\mtrx{Q}= (\mtrx{D} - \omega \mtrx{A})/\tau^2$ is the defining feature of the conditional 
autoregressive (CAR) model.  When $\omega = 1$, and thus $\mtrx{P}= \mtrx{D} - \mtrx{A}$, 
the model is sometimes referred to as an intrinsic CAR model. In general, CAR and (G)MRF 
refer to models with the same structure but different parameterizations. In the literature, the 
term CAR tends to be reserved for the model when expressed in terms of conditional 
distributions (for each random variable given its neighborhood) rather than the joint distribution \shortcite{banerjee_hierarchical_2004}. 
See also Appendix A % Appendix~\ref{AppendixA}
for more details.\label{footnote_car}} 
The $RW_1$ can be extended to an $RW_2$ prior by also considering  
times $t-2$ and $t+2$ as neighbors of $t$. 


\subsection{Hyperpriors}
\label{hyperpriors}

To complete the hierarchical model requires specifying hyperpriors $p(\tau_j^2)$.  
By estimating -- rather than fixing -- the variance hyperparameter $\tau_j^2$, the smoothing 
function $f_j$ and the amount of smoothness are estimated simultaneously. 
\citeA{fahrmeir_bayesian_2001} and \citeA{brezger_star_2005} recommend  
inverse-gamma priors for the $\tau_j^2$. However, in light of concerns about this type 
of inverse-gamma prior raised by \citeA{gelman_prior_2006}, in this thesis several priors 
for $\tau^2_j$ are used and compared to check the degree to which the results are 
sensitive to this choice.

