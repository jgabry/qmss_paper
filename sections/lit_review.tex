
\chapter{Literature review}
\label{lit_review}

\section{Hierarchical Bayesian models}

A hierarchical Bayesian model is simply a model written in hierarchical form and estimated via Bayesian methods. Essentially, such a model can be viewed as the combination of a within-unit model -- e.g., describing  feature of individuals over time -- and an across-unit model -- e.g., describing heterogeneity across individuals.  An alternative yet mathematically equivalent way of conceiving of these models is as an infrastructure linking two or more layers of unknowns: the quantities we are interested in estimating ({\it process variables}) and  the quantities introduced in developing our model ({\it model parameters})

{\singlespacing
$$ \textsc{data} \mid \textsc{process, parameters}$$
$$\textsc{process} \mid \textsc{parameters}$$
$$\textsc{parameters}$$ 
}

\noindent where the notation $A | B$ means $A$ conditional on $B$ (wikle\_bried\_2004). The link between the levels of the hierarchy is Bayes' rule, which states that for data $y$ with likelihood $p(y | \theta)$ and parameter(s) $\theta$ with prior density $p(\theta)$ the posterior distribution of $\theta$ given the data $y$ is proportional to the product of the prior $\theta$ and likelihood: 

{\singlespacing
 $$p(\theta | y ) \propto p(\theta) p(y | \theta). $$
}
%
\noindent Bayes' rule provides the framework for updating beliefs about parameters $\theta$ based on the observed data $y$.  With observations at the base level of the hierarchy it is possible to use Bayes' rule to find the conditional distributions for the quantities/parameters of interest at the upper levels of the hierarchy \shortcite{gelman_bayesian_2013}. 









For example, imagine a series of $J$ experiments with outcomes $\{y_j : j = 1, \dots, J\}$. We want to make inferences about the parameter $\theta_j$ underlying the data generating process for $y_j$. Conditional on the value of the unknown parameter $\theta_j$ we have a likelihood $p(y_j | \theta_j)$ for $y_j$  parameterized by $\theta_j$.  For expositional simplicity, we can assume that parameters $\theta = (\theta_1, \dots, \theta_J)'$ share a common prior distribution with hyperparameter $\phi$. Now, we can either fix $\phi$, or we can estimate $\phi$ by giving it its own prior distribution (a {\it hyperprior}) $p(\phi | \alpha)$, where $\alpha$ is the parameter (or parameters). If we make the choice to estimate $\phi$ then we have a hierarchical model in that we have added another level of variability to the model that is absent if $\phi$ is assumed to be known. We can extend the hierarchy to an arbitrary number of levels by assigning a prior distribution to $\alpha$ and the parameters of that distribution, etc. If $\alpha$ is assumed to be fixed then model is then

{\singlespacing
$$y_j \sim p(y_j | \theta_j)$$
$$\theta_j \sim p(\theta_j | \phi)$$
$$\phi \sim p(\phi | \alpha).$$
}
%
A thorough introduction to Bayesian hierarchical models can be found in \citeA{gelman_bayesian_2013}. 




\section{Gaussian Markov random fields}

The prior distributions of particular interest in this thesis are known as Gaussian Markov random field (GMRF) priors. The literature on GMRFs is vast, as they are frequently used in image processing and spatial statistics, however  GMRFs appear only rarely in quantitative social science and \citeA{wawro_designing_2014} is the first example extending the applications of GMRFs to historical social scientific inquiry. Before defining GMRFs it is first necessary to introduce some basic concepts from graph theory, particular the idea of an undirected graph. 

\subsection{Undirected graphs}
 An undirected graph $G$ is simply an ordered pair of sets $(V,E)$ containing the vertices (also called nodes) and edges of the graph, respectively. In what follows assume that $V$ and $E$ are finite sets and let $e_{ij} \in E$ denote the element in $E$ that corresponds to the edge connecting the vertices $i$ and $j$ in $V$. The term {\it undirected} refers to the fact that the edges lack orientation.\footnote{Intuitively, it is helpful to think of the edges of  an {\it undirected} graph as line segments with no implied direction, whereas the edges in a {\it directed} graph can be thought of as arrows.} The {\it neighbors} of a node $j \in V$ are all nodes $i \neq j$ such that $e_{ij} \in E$. We will denote the set of neighbors of node $j$ by $\partial_j$. For a more formal and thorough discussion of undirected graphs see \citeA{rue_gaussian_2005}. 

\subsection{GMRFs}
A normally distributed random vector $\theta = (\theta_1, \dots, \theta_k) \sim \mathcal{N}_k (\mu, \boldsymbol{\Sigma})$ is said to be a GMRF with respect to a graph $G = (V,E)$ if each element in $\theta$ has a corresponding node in $V$ and there is no edge between a pair of nodes $i$ and $j$ \emph{if and only if} $\theta_i$ and $\theta_j$ are conditionally independent given all other elements of $\theta$. 

%Using the notation from above we thus require that $V = \{1, \dots, k\}$ and $i \notin \partial_j \iff \theta_i \perp \theta_j \mid \theta_{-ij}$. 

The relationship between $G$ and $\theta$ -- the information they provide about each other -- is fully contained in the covariance matrix $\boldsymbol{\Sigma}$.  This must be the case, since from the mean vector $\mu$ nothing can be inferred about conditional independencies among the elements of $\theta$, but it is not obvious from the individual elements of $\boldsymbol{\Sigma}$. It is more useful to instead consider the precision matrix $\boldsymbol{Q}=\boldsymbol{\Sigma}^{-1}$, for which it can be shown that

\begin{equation*}
\forall i \neq j, \:\: v_i \in \partial_j \iff q_{ij} = 0,
\end{equation*}

\noindent which is to say that conditional independence between $\theta_i$ and $\theta_j$ (no edge between nodes $i$ and $j$) always corresponds to a zero in cell $ij$ of the precision matrix, and vice-versa.  For a simple proof of this result \citeA{rue_gaussian_2005}.\footnote{For intuition, the following interpretations of the elements of a precision matrix may be helpful to keep in mind. For $i \neq j$ (the off-diagonal), $q_{ij}  = -Cov(\theta_i, \theta_j | \theta_{-ij}) $, the negative (flipped sign) covariance between $\theta_i$ and $\theta_j$ conditional on all $\theta$s except $i$ and $j$.  For $i = j$ (the diagonal),  $q_{ij} = Var(\theta_i | \theta_{-i})$, the variance of $\theta_i$ conditional all the variables except $\theta_i$.}






\section{Bayesian STAR models}

An extension of generalized linear models (GLMs), semiparametric structured additive regression (STAR) models replace the linear predictor with a structured additive predictor of the form
%
\begin{equation*}
\eta = f_1(x_1) + \ldots + f_j(x_j) + \ldots + f_J(x_J) + u'\gamma,
\end{equation*}
%
\noindent which can include nonlinear unknown functions of covariates as well as linear components \shortcite{fahrmeir_bayesian_2001}. From the Bayesian perspective, the non-varying parameters $\gamma$ as well as the unknown functions $f_1, \dots, f_J$ are treated as random variables distributed according to priors that we must specify. More specifically, if we have $N$ observations, then for an unknown function $f_j$ of a covariate $x_j$ we treat the vector of evaluations $\mathbf{f}_j^{eval} = \left(f_j(x_{1j}), \dots f_j(x_{Nj})\right)' $ as a random vector. Following \citeA{brezger_generalized_2006}, we can conviently express $\mathbf{f}_j^{eval}$ as the matrix product $\mathbf{M}_j \theta_j$ of a design matrix $\mathbf{M}_j$ and a parameter vector $\theta_j$. GMRF priors for each parameter vector $\theta_j$ take the general form

\begin{equation*}
p(\theta_j | \tau^2_j) 
\propto 
\frac{1}{\left(\tau^2_j\right)^{{\rm rank}(\mathbf{P})/2} }
\exp{\left\{-\frac{1}{2\tau^2_j} \theta_j' \mathbf{P} \theta_j\right\}}
\end{equation*}

\noindent where $\mathbf{P}$ is a penalty matrix whereby we operationalize our prior assumptions about the smoothness of the unknown function $f_j$. 

To provide some intuition, suppose we are interested in learning about an effect with assumed variation over geographic regions.  A simple map with $R$ distinct regions $r_1, r_2, \dots r_R$ can be represented as undirected graph $G$ with vertices $V = \{1, 2, \dots, R\}$ and edges connecting vertices corresponding to neighboring regions. To express prior beliefs about the spatial dependence of the quantity of interest between regions we can construct a prior on a parameter vector $\theta = (\theta_1, \dots, \theta_r)$ such that the matrix $\mathbf{P}$ has a zero in the $ij$th cell if and only if $\theta_i$ and $\theta_j$ are assumed to be conditionally independent given $\theta_{-ij}$. Then $\theta$ is a GMRF with respect to $G$ with $\boldsymbol{\Sigma}^{-1} = \mathbf{Q} = \mathbf{P}/\tau^2$. 

\subsection{The penalty matrix} 
\label{penalty_matrix}

More specifically, the penalty matrix $\mathbf{P}$ can be constructed as  $\mathbf{P} = \mathbf{D} - \mathbf{A}$, where $\mathbf{A}$ is a symmetric matrix with $a_{ij} = 1$ if temporal or spatial units $i$ and $j$ are considered neighbors (the associated graph $G$ has an edge between nodes $i$ and $j$) and 0 otherwise, and $\mathbf{D}$ is a diagonal matrix such that $\forall i = j, \: d_{ij} = \sum_j a_{ij}$. The matrices $\mathbf{A}$ and $\mathbf{D}$ are commonly referred to as the adjacency and degree matrices because encoded in $\mathbf{A}$ are all neighbor relationships (temporal or spatial) and the diagonal elements of $\mathbf{D}$ are the number of neighbors (the degree) of each vertex in the graph $G$.

To illustrate why this form of $\mathbf{P}$ captures these particular assumptions, consider $N$ measurements of a variable $x$, with each measurement of $x$ made at one of $T$ evenly spaced points in time. For simplicity, but without loss of generality,  assume that $x$ is a unit of time and there is exactly one measurement per time period.  The sequence $(x^{[t]})_{t=1}^T$ then corresponds to a grid of points on a line.  \citeA{fahrmeir_bayesian_2001} suggest several possible choices for a prior on a smooth function $f(x)$, the simplest of which is a first order random walk ($RW_1$) prior.  Under the $RW_1$ prior, the first differences $\Delta_t = f(x^{[t]}) - f(x^{[t-1]})$ are treated as independent and identically distributed standard normal random variables. 

While this formulation of the $RW_1$ prior is {\it directed}, conditioning also on $f(x^{[t+1]})$ -- one step into the future -- forms an undirected $RW_1$, where the neighbors of time $t$ are both $t-1$ and $t+1$.  The associated graph $G$ therefore has vertices $V=\{v_t : t=1,\dots,T\}$, each of which has two neighbors, with the exception of $v_1$ and $v_T$, which have one neighbor. The penalty matrix $\mathbf{P}$ corresponding to the $RW_1$ prior with equally spaced observations is the tridiagonal matrix

\begin{equation*}
\mathbf{P} = 
\begin{bmatrix}
1  	& -1 	& 		& 	& \\
-1  	& 2 	& -1 		& 	& \\
  	& -1 	& \ddots 	& \ddots	& \\
  	&  	& \ddots 	& 2 	& -1\\
  	&  	& 		& -1 	& 1\\
\end{bmatrix}
\end{equation*}

\noindent which can be derived by computing the difference of the appropriate degree and adjacency matrices \shortcite{brezger_generalized_2006}.\footnote{The construction of $\mathbf{P}$ as the matrix difference $\mathbf{D} - \mathbf{A}$ also allows for the estimation of a parameter $\omega \in [0,1]$, which, as a coefficient on $\mathbf{A}$, can be interpreted as representing the strength of spatial or temporal dependence \shortcite{rue_gaussian_2005}. The resulting precision matrix $\mathbf{Q}= (\mathbf{D} - \omega \mathbf{A})/\tau^2$ is the defining feature of the conditional autoregressive (CAR) model.  When $\omega = 1$, and thus $\mathbf{P}= \mathbf{D} - \mathbf{A}$, the model is sometimes referred to as an intrinsic CAR model.
} The $RW_1$  can be extended to an $RW_2$ prior by also considering the measurements $x^{[t-2]}$  and $x^{[t+2]}$ to be neighbors of $x^{[t]}$. 


\subsection{Hyperpriors}
\label{hyperpriors}

To complete the hierarchical model requires also specifying hyperpriors $p(\tau_j^2)$.  Assigning a prior distribution to the variance hyperparameter $\tau_j^2$ allows for the simultaneous estimation of a smoothing function $f_j$ and the amount of smoothness. \citeA{fahrmeir_bayesian_2001} and \citeA{brezger_bayesx:_2005} recommend a weakly informative but proper inverse-gamma prior on $\tau_j^2$. However, in light of concerns about this type of inverse-gamma prior raised by \citeA{gelman_prior_2006}, in this thesis several priors for $\tau^2_j$ are used and compared to check the degree to which the results are sensitive to the choice of prior.

