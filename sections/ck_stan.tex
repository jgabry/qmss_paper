 \subsection{Estimation using Stan}
 \label{stan_intro}

For notational convenience, let $x$ represent the observed $n$ (number of roll-call votes) 
and $v^{Ratio}$ (ratio of majority to minority average vote-share).  And let 
$\psi = \big((\tau^2_\lambda, \bar{\lambda}), (\tau^2_\rho, \bar{\rho}), \phi \big)$ denote 
the hyperparameters of the model, and $\bm{f}$ the unknown functions  $f_\lambda$ and 
$f_\rho$. The joint posterior distribution for $(\bm{f}, \psi)$ given the data $(y, x)$ can then 
be more concisely expressed as 

\begin{align*}
p(\bm{f}, \psi | y, x) 
&\propto p(\bm{f}, \psi)  p(y | x, \bm{f}, \psi)  \\
&=p(\psi)  p(\bm{f} | \psi)   \prod_{n=1}^N p(y_n | \eta_n, x_n), 
\end{align*}

\noindent where the second line follows from assumptions of independence and conditional 
independence.\footnote{In particular it is assumed that hyperparameters are mutually independent 
in their priors, $p(f_\lambda | \tau^2_\lambda, \bar{\lambda})$ and $p(f_\rho | \tau^2_\rho, \bar{\rho})$ 
are conditionally independent, and the observations are independent conditional on parameters 
and $x$.}

Estimation is performed by Markov chain Monte Carlo (MCMC) approximation and implemented in 
RStan, the R interface to the probabilistic programming language and C++ library Stan 
\shortcite{rstan_software:2015}. Although other existing software including BayesX, JAGS, and 
OpenBugs can sometimes be used to fit similar models by MCMC, each has important limitations 
that Stan overcomes. Several of the many advantages to using Stan are discussed in the next section. 
Since there are few published examples of fitting similar models in Stan, the next section is 
written in the style of a technical vignette, introducing Stan and describing how the model is coded 
and estimated. 

\subsubsection{Brief introduction to Stan}

\citeA{wawro_designing_2014} uses software called BayesX \shortcite{bayesx_software}, which 
is appropriate and  efficient for certain models, but offers limited flexibility in the choice of priors, 
likelihoods, and link functions. To make the recommendations of Wawro and Katznelson more widely 
applicable will require software with greater versatility. \citeA{goodrich_designing_2012} advocates 
using Stan, which can fit a more diverse range of models than is currently possible with BayesX 
and other software packages, and allows for analyses in the spirit of \citeA{wawro_designing_2014} 
to be extended beyond the confines of their particular examples.  

Stan is a probabilistic modeling language, optimizer, and MCMC sampler. The particular MCMC 
algorithm implemented in Stan is a variant of Hamiltonian Monte Carlo (HMC) called the No-U-Turn 
Sampler (NUTS) \shortcite{hoffman_2012}, designed for fitting rich Bayesian statistical models. 
Borrowing from physics the concepts and mathematics behind Hamiltonian dynamics, HMC treats 
the vector of unknown parameters as the position of a particle. In Hamiltonian dynamics, momentum 
and position change continuously over time, with the gradient of the particle's potential energy function 
--  which corresponds to the negative log posterior -- responsible for changes in momentum, 
and momentum governing changes in position. Stan works by simulating a discretization of this 
process, making necessary corrections to satisfy the detailed balance equations (that is, to ensure 
the resulting Markov chains are reversible).\footnote{Reversibility of the Markov chain is important 
because every reversible Markov chain has a stationary distribution (reversibility is sufficient 
but not necessary). Intuitively, reversibility means that for $x$ and $x^\prime$ generated 
from the target density, the probability of a transition from state $x$ to $x^\prime$ 
is the same as for $x'$ to $x$.}  

Several characteristics of HMC, and in particular Stan's implementation of HMC, make it a more 
appealing choice than traditional Metropolis (and variants) and Gibbs samplers, especially for
high-dimension complex models \shortcite{hoffman_2012, stan_development_team_stan_2015}. 
Metropolis algorithms require considerable customization and place the 
burden of tuning the sampler on the user. Gibbs samplers typically involve less tuning, but require 
specifying full conditional distributions for all parameters. Except in a limited number of cases, full 
conditionals are difficult or impossible to derive, which results in conjugate priors being used when 
other priors might be more believable or preferable for other reasons. Both suffer from random walk 
behavior that leads to inefficient exploration of the parameter space, a problem exacerbated 
by hierarchical models with complex structures \shortcite{betancourt_hamiltonian_2013}.

Stan is designed to overcome each of these concerns. The random walk behavior that plagues both 
Gibbs and Metropolis is largely avoided because HMC uses gradient information to find posterior modes 
much more efficiently, thus greatly reducing the number of iterations required to obtain a sufficient 
effective posterior sample size \shortcite{hoffman_2012}.\footnote{Stan computes gradients using 
reverse-mode automatic differentiation. A paper on the details of Stan's especially efficient 
implementation of automatic differentiation is forthcoming.}  Although the theory behind HMC does 
not obviate the need for the user to specify tuning parameters, Stan's NUTS-based adaptation algorithm 
attempts to learn appropriate values of all such parameters (without user input) during a warmup 
period before sampling.\footnote{While the default settings are typically fine, for particularly complicated 
models it can sometimes be helpful for users to manually adjust tuning parameters to improve the 
performance of the algorithm. See \citeA{stan_development_team_stan_2015} for examples.} 
Finally, there is no advantage to conjugacy in Stan. 
Users are free to specify prior distributions that more accurately reflect current knowledge. 
For a more thorough introduction to Stan and NUTS see \citeA{stan_development_team_stan_2015} 
and \citeA{hoffman_2012}, respectively.\footnote{See also Appendix D %Appendix~\ref{AppendixD} 
for a simplified version of the HMC algorithm used by Stan.}

\subsubsection{Stan program: data and transformed data}

In the {\tt data} block of a Stan program (Figure~\ref{stan_data}) we declare the data that will 
be passed to Stan. In this case, the declarations are all straightforward, defining the dimensions 
and variables discussed in section~\ref{ckdata}. The exception is {\tt Pinverse}, the inverse of 
the penalty matrix, which is precomputed and passed in as data.\footnote{All R and Stan code 
will be made publicly available on GitHub.}

\begin{figure}[h]
\begin{lstlisting}[language=Stan, frame=trBL]
data {
  // dimensions 
  int<lower=1>          N ; # number of observations 
  int<lower=1>          C ; # number of congresses (time periods)
  
  // arrays of variables 
  int<lower=1,upper=C>  cong[N] ;     # maps between obs & congress
  int<lower=1,upper=56> nVotes[N] ;   # number of votes
  int<lower=0,upper=55> nWins[N] ;    # number of maj party victories
  real                  lvRatio[N] ;  # log(vRatio)
  
  // inverse of penalty matrix 
  matrix[C,C]           Pinverse ; # for GMRF prior
}
transformed data {
  real<lower=0> phi_scale ;     # scale for prior on phi
  real<lower=0> phi_loc ;       # location for prior on phi
  real<lower=0> tau_scale ;     # scale for priors on taus
  real<lower=0> tau_loc ;       # location for priors on taus
  matrix[C,C]   cholPinverse ;  # Cholesky decomposition 
  
  phi_loc <- 0 ;
  phi_scale <- 10 ;
  tau_loc <- 0 ;
  tau_scale <- 1 ;
  cholPinverse <- cholesky_decompose(Pinverse) ;
}
\end{lstlisting}
\caption{Stan code: {\tt data} and {\tt transformed\_data} blocks}
\label{stan_data}
\end{figure}



The {\tt transformed data} block (also Figure~\ref{stan_data}) contains transformations of the 
variables declared in the {\tt data} block and defines other fixed quantities to be used throughout 
the rest of the program. The Cholesky decomposition of {\tt Pinverse} is computed and will be used 
for a more efficient implementation of the multivariate normal distributions required for the GMRF 
priors. Values for the location and scale parameters of the Cauchy and normal distributions 
(to be used for priors) are also set in {\tt transformed data}. 


\subsubsection{Stan program: parameters and transformed parameters}

In the {\tt parameters} and {\tt transformed parameters} blocks (Figures~\ref{stan_parameters} 
and \ref{stan_trans_parameters}) we declare model parameters and deterministic transformations 
of the declared parameters. Stan tends to work best if the target posterior distribution is marginally 
standard normal and uncorrelated and parameters are on a similar scale. For this reason it can help 
improve efficiency and convergence if variables defined in {\tt parameters} are given standard normal 
priors when possible and then transformed in {\tt transformed parameters} to have the desired 
distribution.\footnote{This is an extremely oversimplified description of an important issue, but 
a thorough examination is beyond the scope of this thesis. See \citeA{betancourt_hamiltonian_2013} 
and \citeA{stan_development_team_stan_2015}.} 

\begin{figure}[h]
\begin{lstlisting}[language=Stan, frame=trBL]
parameters {
  real<lower=0> phi_noise ;    
  real          lambda_bar ;  # prior mean 
  real<lower=0> rho_bar ;     # prior mean 
  vector[C]     lambda_noise ;      
  vector[C]     rho_noise ;
  real<lower=0> tau_sq_lambda ;  
  real<lower=0> tau_sq_rho ;
}
\end{lstlisting}
\caption{Stan code: {\tt parameters} block}
\label{stan_parameters}
\end{figure}

In {\tt transformed parameters}, ${\tt phi\_noise}$ is transformed from a standard normal 
distribution using the inverse-CDF method such that ${\tt phi}$ has a half-Cauchy 
distribution.\footnote{
The inverse cumulative distribution function of the Cauchy distribution with location $\gamma$ 
and scale $\zeta$ is $F^{-1}(p, \gamma,\zeta)  = \gamma + \zeta \tan{\{ \pi (p - 1/2)\}}$. Thus if 
$z \sim \mathcal{N}(0,1)$ with CDF $\Phi$ then $\gamma + \zeta \tan{\{ \pi (\Phi(z) - 1/2)\}}$ is 
distributed Cauchy$(\gamma, \zeta)$. The transformation above results in a half-Cauchy$({\tt phi\_loc}, 
{\tt phi\_scale})$ distribution due to the constraints that ${\tt phi\_noise}$ and ${\tt phi}$ be positive.}\footnote{
The Cauchy prior can be considered weakly informative in that, while most of the mass is concentrated 
near the median, the tails are fat enough to allow for considerable variation. In fact, the variance of a 
Cauchy distribution is infinite.} The vectors ${\tt lambda\_noise}$ and ${\tt rho\_noise}$ represent variables 
to be given iid standard normal priors, which allows for the non-centered parameterization of the 
multivariate normal to be used for ${\tt lambda}$ and ${\tt rho}$.\footnote{
The non-centered parameterization works as follows. If $\zeta \in \mathbb{R}^D$ is a vector of i.i.d 
$\mathcal{N}(0,1)$ variables and $\theta = \mu + L \zeta$, where 
$LL' = \boldsymbol{\Sigma} \in R^{D \times D}$, then $\theta \sim \mathcal{N}_D (\mu, \boldsymbol{\Sigma})$.
This is analogous to the familiar univariate case where $\theta = \mu + \sigma \zeta$ and 
$\zeta \sim \mathcal{N}(0,1)$ implies that $\theta \sim \mathcal{N}(\mu, \sigma^2)$.} Unlike the elements 
of ${\tt lambda}$ and ${\tt rho}$, the elements of ${\tt lambda\_noise}$ and ${\tt rho\_noise}$ are 
independent by construction, which can lead to large gains in efficiency for MCMC algorithms in 
terms of effective sample size \shortcite{stan_development_team_stan_2015, betancourt_hamiltonian_2013}. 

\begin{figure}
\begin{lstlisting}[language=Stan, frame=trBL]
transformed parameters {
  real<lower=0> phi ;
  vector[C]     lambda ;
  vector[C]     rho ;
  
  // inverse CDF method, standard normal --> half-Cauchy
  phi <- phi_loc + phi_scale * tan(pi() * (Phi_approx(phi_noise) - 0.5)) ;
  // non-centered parameterization for lambda and rho
  lambda  <- lambda_bar + (tau_sq_lambda * cholPinverse) * lambda_noise ;
  rho  <- rho_bar + (tau_sq_rho * cholPinverse) * lambda_noise ;
}
\end{lstlisting}
\caption{Stan code: {\tt transformed parameters} block}
\label{stan_trans_parameters}
\end{figure}



\begin{figure}[p]
\begin{lstlisting}[language=Stan, frame=trBL]
model {
  // local/temporary variables
  real                logLik ;    # log likelihood
  real                logPrior ;  # log prior
  vector<lower=0>[N]  alphas ;    # shape1 for beta_binomial 
  vector<lower=0>[N]  betas ;     # shape2 for beta_binomial
  
  // priors
  logPrior <- (
    normal_log(lambda_bar, 0, 1) + 
    normal_log(rho_bar, 4, 2) + 
    normal_log(phi_noise, 0, 1) +
    normal_log(tau_sq_lambda, tau_loc, tau_scale) +
    normal_log(tau_sq_rho, tau_loc, tau_scale) +
    normal_log(lambda_noise, 0, 1) +  # vectorized
    normal_log(rho_noise, 0, 1)       # vectorized
  ) ;
  
  // likelihood
  for (n in 1:N) {
    real eta_n ;
    real theta_n ;
    eta_n <- lambda[cong[n]] + rho[cong[n]] * lvRatio[n] ;
    theta_n   <- inv_logit(eta_n) ;    
    alphas[n] <- theta_n * phi ;
    betas[n]  <- (1 - theta_n) * phi ;
  }
  
  // vectorized BetaBinomial
  logLik <- beta_binomial_log(nWins, nVotes, alphas, betas) ; 
  
  // logPrior + logLik = logPosterior (up to normalizing constant)
  increment_log_prob(logPrior + logLik) ; 
}
\end{lstlisting}
\caption{Stan code: {\tt model} block}
\label{stan_model}
\end{figure}
%


\subsubsection{Stan program: model}

In the {\tt model} block (Figure~\ref{stan_model}) the likelihood and priors are specified. More 
precisely, Stan requires the log-likelihood and log-priors, the sum of which equals the log-posterior 
up to an additive constant.\footnote{Stan works on the log scale for the same reason as most 
other statistical software. Logarithms help avoid the loss of the numerical precision in addition to 
greatly simplifying expressions for complicated probability distributions.}
The priors include the standard normals on ${\tt phi\_noise}$, ${\tt lambda\_noise}$ and ${\tt rho\_noise}$ 
discussed above, and normal priors for ${\tt lambda\_bar}$, ${\tt rho\_bar}$, ${\tt tau\_sq\_lambda}$, 
and ${\tt tau\_sq\_rho}$.\footnote{The function names in {\tt model} 
ending in ${\tt\_log}$ return the logarithm of the density function (e.g. ${\tt normal\_log(theta,0,1)}$ 
returns the logarithm of the normal density of ${\tt theta}$  with location 0 and scale 1). While Stan  
supports the familiar sampling statement notation (e.g. ${\tt theta \sim normal(0,1)}$), it is not 
used here because such notation is somewhat misleading. A sampling statement like 
${\tt theta \sim normal(0,1)}$ does {\it not} result in ${\tt theta}$ being drawn from the standard 
normal distribution. Rather, it means that the logarithm of the standard normal density (up to a constant) 
evaluated at ${\tt theta}$ will be added to the total log probability accumulator. This can be expressed 
explicitly with ${\tt increment\_log\_prob(normal\_log(theta,0,1))}$, which more accurately reflects 
Stan's execution under the hood. The only non-cosmetic difference between sampling statements and directly 
incrementing the log probability is that the former drops the unneeded constant terms. For a more detailed 
explanation see \citeA{stan_development_team_stan_2015}.}  

At the top of the {\tt model} block two vectors, {\tt alpha} and {\tt beta}, are also declared 
and will be filled in while looping over observations. This allows for the BetaBinomial likelihood 
to be vectorized for faster computation. Expressing the likelihood in this way is perhaps less 
intuitive than incrementing the likelihood within the loop (and without creating the temporary 
{\tt alpha} and {\tt beta} variables), for example

\begin{lstlisting}[language=Stan, basicstyle=\footnotesize\singlespacing, backgroundcolor=]
  logLik <- 0.0 ;
  for (n in 1:N) {
   ...
   logLik <- logLik +
   		beta_binomial_log(nWins[n], nVotes[n], 
   				  theta_n * phi, (1-theta_n) * phi) ;
  }
\end{lstlisting}
%
\noindent but filling in the elements of {\tt alpha} and {\tt beta} inside the loop and then using the 
vectorized version of the BetaBinomial is much more computationally effective. 

Note also that in the {\tt model} block prior distributions are placed only on variables declared 
in {\tt parameters}  and not those defined in {\tt transformed parameters}. This is not a requirement, 
but assigning distributions to variables declared in {\tt transformed parameters} demands the 
additional -- and potentially onerous -- step of accounting for changes in curvature due to the 
change of variables. For scalar parameters this entails calculating the log absolute derivative of 
the transform and incrementing the log probability by this value. For multivariate changes of variables 
the log probability must be incremented by the log absolute determinant of the corresponding 
Jacobian matrix.\footnote{This is explained in detail and with examples in 
\citeA{stan_development_team_stan_2015}. In brief, the adjustment accounts for how 
the scale of the variable obtained by the transformation varies with respect to the 
original variable.}



\subsubsection{Stan program: generated quantities}

The {\tt generated quantities} block (Figure~\ref{stan_generated_quantities}) allows for the computation 
of distributions of quantities of interest without affecting the log posterior specified in the model block. 
In this case the estimated bias towards the majority party in each congress is computed. For model 
checking (Appendix F %Appendix~\ref{AppendixF}
), simulations from the posterior predictive distribution 
are also generated.\footnote{The call to ${\tt beta\_binomial\_rng}$ is inside the loop in the 
{\tt generated quantities} block unlike the call to ${\tt beta\_binomial\_log}$ in the {\tt model} block because 
random number generating functions are not currently vectorized in Stan.} 


\begin{figure}[h]
\begin{lstlisting}[language=Stan, frame=trBL]
generated quantities {
  real        bias[C] ;  # bias toward majority party
  int         y_rep[N] ; # draws from posterior predictive distribution
  
  for (c in 1:C) 
    bias[c] <- inv_logit(lambda[c]) - 0.5 ;
  
  for (n in 1:N) {
    // local variables reassigned each pass through the loop
    real eta_n ;
    real theta_n ;
    real alpha_n ;
    real beta_n ;
    eta_n <- lambda[cong[n]] + rho[cong[n]] * lvRatio[n] ;
    theta_n <- inv_logit(eta_n) ;    
    alpha_n <- phi * theta_n ;
    beta_n  <- phi * (1 - theta_n) ;
    // draw from posterior predictive distribution
    y_rep[n] <- beta_binomial_rng(nVotes[n], alpha_n, beta_n) ;
  }
}
\end{lstlisting}
\caption{Stan code: {\tt generated quantities} block}
\label{stan_generated_quantities}
\end{figure}

\clearpage

